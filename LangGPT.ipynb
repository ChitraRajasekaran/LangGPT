{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acb9f5-70f5-4631-aaa6-a6fdaa26cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28997434-67f3-4873-84d4-46dd4c87e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "# !pip install datasets\n",
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60a12c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(r\"/home/vignesh/anaconda3/lib/python3.9/site-packages/datasets\")\n",
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05c6fa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a736099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Assign device value as \"cuda\" to train on GPU if GPU is available. Otherwise it will fall back to default as \"cpu\".\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c17154-32f0-4303-97ed-4f676b9ed5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc01ce24-b0c2-4703-a280-e2dc5b47405b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading train, validation, test dataset from huggingface path below.\n",
    "raw_train_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ta\", split='train')\n",
    "raw_validation_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ta\", split='validation')\n",
    "raw_test_dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ta\", split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e822ec9-7cb7-427d-a4cd-267e176bf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store dataset files.\n",
    "os.mkdir(\"./dataset-en\")\n",
    "os.mkdir(\"./dataset-ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb2e8e57-09b2-47ff-be05-a1531a1bd6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to save model during model training after each EPOCHS (in step 10).\n",
    "os.mkdir(\"./tamilgpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b7060d7-8d86-4578-a876-97928a5d1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Director to store source and target tokenizer.\n",
    "os.mkdir(\"./tokenizer_en\")\n",
    "os.mkdir(\"./tokenizer_ta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f415ed86-b522-41c9-8fa7-8bdf95022a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_en = [] \n",
    "dataset_ta = []\n",
    "file_count = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fae0089a-58fb-442a-be25-9fdad45e5008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 227014/227014 [00:00<00:00, 769675.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# In order to train the tokenizer (in step 2), we'll separate the training dataset into english and tamil. \n",
    "# Create multiple small file of size 20k data each and store into dataset-en and dataset-ta directory.\n",
    "for data in tqdm(raw_train_dataset[\"translation\"]):\n",
    " dataset_en.append(data[\"en\"].replace('\\n', \" \"))\n",
    " dataset_ta.append(data[\"ta\"].replace('\\n', \" \"))\n",
    " if len(dataset_en) == 20000:\n",
    "     with open(f'./dataset-en/file{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "         fp.write('\\n'.join(dataset_en))\n",
    "         dataset_en = []\n",
    "    \n",
    "     with open(f'./dataset-ta/file{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "         fp.write('\\n'.join(dataset_ta))\n",
    "         dataset_ta = []\n",
    "         file_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a35cfb0-0bca-4d05-be52-5937bfc07c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 2: Create Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfa520c8-f179-4128-a0ee-c9059094d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tokenzier library classes and modules.\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be675bfe-9c19-4f5b-84f5-e13d7194bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# path to the training dataset files which will be used to train tokenizer.\n",
    "path_en = [str(file) for file in Path('./dataset-en').glob(\"**/*.txt\")]\n",
    "path_ta = [str(file) for file in Path('./dataset-ta').glob(\"**/*.txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69b4ace8-a3ee-445d-a8df-375cd766b8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ Creating Source Language Tokenizer - English ].\n",
    "# Additional special tokens are created such as [UNK] - to represent Unknown words, [PAD] - Padding token to maintain same sequence length across the model.\n",
    "# [CLS] - token to denote start of sentence, [SEP] - token to denote end of sentence.\n",
    "tokenizer_en = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer_en = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3a186f-74e3-47a9-87a8-5c96aa57b69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting tokens based on whitespace.\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Tokenizer trains the dataset files created in step 1\n",
    "tokenizer_en.train(files=path_en, trainer=trainer_en)\n",
    "\n",
    "# Save tokenizer for future use.\n",
    "tokenizer_en.save(\"./tokenizer_en/tokenizer_en.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49846cd5-72e8-4894-9e0f-0991f91c5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ Creating Target Language Tokenizer - Tamil ].\n",
    "tokenizer_ta = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer_ta = BpeTrainer(min_frequency=2, special_tokens=[\"[PAD]\",\"[UNK]\",\"[CLS]\", \"[SEP]\", \"[MASK]\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa3e243-85e4-44ec-96c4-6f18367247ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer_ta.pre_tokenizer = Whitespace()\n",
    "tokenizer_ta.train(files=path_ta, trainer=trainer_ta)\n",
    "tokenizer_ta.save(\"./tokenizer_ta/tokenizer_ta.json\")\n",
    "\n",
    "tokenizer_en = Tokenizer.from_file(\"./tokenizer_en/tokenizer_en.json\")\n",
    "tokenizer_ta = Tokenizer.from_file(\"./tokenizer_ta/tokenizer_ta.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def75b98-818a-4d29-9ebf-833c88f9b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting size of both tokenizer.\n",
    "source_vocab_size = tokenizer_en.get_vocab_size()\n",
    "target_vocab_size = tokenizer_ta.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c702f5-8fb9-4b81-a1c4-b1529bee651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(source_vocab_size)\n",
    "print(target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ab779dc-55e6-44f4-b817-8b6faf4d5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define token-ids variables, we need this for training model.\n",
    "CLS_ID = torch.tensor([tokenizer_ta.token_to_id(\"[CLS]\")], dtype=torch.int64).to(device)\n",
    "SEP_ID = torch.tensor([tokenizer_ta.token_to_id(\"[SEP]\")], dtype=torch.int64).to(device)\n",
    "PAD_ID = torch.tensor([tokenizer_ta.token_to_id(\"[PAD]\")], dtype=torch.int64).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b947cc18-bf7d-49a7-91d7-9ac85f85c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Prepare Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc4cd58c-f8a6-40dc-97bb-8b0a26765a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_seqlen_source: 665\n",
      "max_seqlen_target: 978\n"
     ]
    }
   ],
   "source": [
    "# This class takes raw dataset and max_seq_len (maximum length of a sequence in the entire dataset).\n",
    "class EncodeDataset(Dataset):\n",
    " def __init__(self, raw_dataset, max_seq_len):\n",
    "     super().__init__()\n",
    "     self.raw_dataset = raw_dataset\n",
    "     self.max_seq_len = max_seq_len\n",
    " \n",
    " def __len__(self):\n",
    "     return len(self.raw_dataset)\n",
    "\n",
    " def __getitem__(self, index):\n",
    " \n",
    "     # Fetching raw text for the given index that consists of source and target pair.\n",
    "     raw_text = self.raw_dataset[index]\n",
    "     \n",
    "     # Separating text to source and target text and will be later used for encoding.\n",
    "     source_text = raw_text[\"en\"]\n",
    "     target_text = raw_text[\"ta\"]\n",
    "    \n",
    "     # Encoding source text with source tokenizer(tokenizer_en) and target text with target tokenizer(tokenizer_ta).\n",
    "     source_text_encoded = torch.tensor(tokenizer_en.encode(source_text).ids, dtype = torch.int64).to(device) \n",
    "     target_text_encoded = torch.tensor(tokenizer_ta.encode(target_text).ids, dtype = torch.int64).to(device)\n",
    "    \n",
    "     # To train the model, the sequence lenth of each input sequence should be equal max seq length. \n",
    "     # Hence additional number of padding will be added to the input sequence if the length is less than the max_seq_len.\n",
    "     num_source_padding = self.max_seq_len - len(source_text_encoded) - 2 \n",
    "     num_target_padding = self.max_seq_len - len(target_text_encoded) - 1 \n",
    "    \n",
    "     encoder_padding = torch.tensor([PAD_ID] * num_source_padding, dtype = torch.int64).to(device)\n",
    "     decoder_padding = torch.tensor([PAD_ID] * num_target_padding, dtype = torch.int64).to(device)\n",
    "     \n",
    "     # encoder_input has the first token as start of sentence - CLS_ID, followed by source encoding which is then followed by the end of sentence token - SEP.\n",
    "     # To reach the required max_seq_len, addition PAD token will be added at the end. \n",
    "     encoder_input = torch.cat([CLS_ID, source_text_encoded, SEP_ID, encoder_padding]).to(device) \n",
    "    \n",
    "     # decoder_input has the first token as start of sentence - CLS_ID, followed by target encoding.\n",
    "     # To reach the required max_seq_len, addition PAD token will be added at the end. There is no end of sentence token - SEP in decoder_input.\n",
    "     decoder_input = torch.cat([CLS_ID, target_text_encoded, decoder_padding ]).to(device) \n",
    "     \n",
    "     # target_label has the first token as target encoding followed by end of sentence token - SEP. There is no start of sentence token - CLS in target label.\n",
    "     # To reach the required max_seq_len, addition PAD token will be added at the end. \n",
    "     target_label = torch.cat([target_text_encoded,SEP_ID,decoder_padding]).to(device) \n",
    "     \n",
    "     # As we've added extra padding token with input encoding, during training, we don't want this token to be trained by model as there is nothing to learn in this token.\n",
    "     # So, we'll use encoder mask to nullify the padding token value prior to calculating output of self attention in encoder block.\n",
    "     encoder_mask = (encoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int().to(device) \n",
    "     \n",
    "     # We also don't want any token to get influenced by the future token during the decoding stage. Hence, Causal mask is being implemented during masked multihead attention to handle this. \n",
    "     decoder_mask = (decoder_input != PAD_ID).unsqueeze(0).unsqueeze(0).int() & causal_mask(decoder_input.size(0)).to(device) \n",
    "\n",
    "     return {\n",
    "     'encoder_input': encoder_input,\n",
    "     'decoder_input': decoder_input,\n",
    "     'target_label': target_label,\n",
    "     'encoder_mask': encoder_mask,\n",
    "     'decoder_mask': decoder_mask,\n",
    "     'source_text': source_text,\n",
    "     'target_text': target_text\n",
    "     }\n",
    "\n",
    "# Causal mask will make sure any token that comes after the current token will be masked, meaning the value will be replaced by -ve infinity which will be converted to zero or close to zero after softmax function. \n",
    "# Hence the model will just ignore these value or willn't be able to learn anything from these values.\n",
    "def causal_mask(size):\n",
    " # dimension of causal mask (batch_size, seq_len, seq_len)\n",
    "    mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "    return mask == 0\n",
    "# To calculate the max sequence lenth in the entire training dataset for the source and target dataset.\n",
    "max_seq_len_source = 0\n",
    "max_seq_len_target = 0\n",
    "\n",
    "for data in raw_train_dataset[\"translation\"]:\n",
    " enc_ids = tokenizer_en.encode(data[\"en\"]).ids\n",
    " dec_ids = tokenizer_ta.encode(data[\"ta\"]).ids\n",
    " max_seq_len_source = max(max_seq_len_source, len(enc_ids))\n",
    " max_seq_len_target = max(max_seq_len_target, len(dec_ids))\n",
    "\n",
    "print(f'max_seqlen_source: {max_seq_len_source}') \n",
    "print(f'max_seqlen_target: {max_seq_len_target}')\n",
    "\n",
    "# To simplify the training process, we'll just take single max_seq_len and add 20 to cover the additional length of tokens such as PAD, CLS, SEP in the sequence.\n",
    "max_seq_len = 400\n",
    "\n",
    "# Instantiate the EncodeRawDataset class and create the encoded train and validation-dataset.\n",
    "train_dataset = EncodeDataset(raw_train_dataset[\"translation\"], max_seq_len)\n",
    "val_dataset = EncodeDataset(raw_validation_dataset[\"translation\"], max_seq_len)\n",
    "\n",
    "# Creating DataLoader wrapper for both training and validation dataset. This dataloader will be used later stage during training and validation of our LLM model.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size = 10, shuffle = True, generator=torch.Generator(device='cuda'))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size = 1, shuffle = True, generator=torch.Generator(device='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4ebb731-777a-4433-81d2-74b801674183",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEP 4: Input embedding and positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5810f06f-c3d0-420c-ad50-481309b62e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input embedding and positional encoding\n",
    "class EmbeddingLayer(nn.Module):\n",
    " def __init__(self, vocab_size: int, d_model: int):\n",
    "     super().__init__()\n",
    "     self.d_model = d_model\n",
    "     \n",
    "     # Using pytorch embedding layer module to map token id to vocabulary and then convert into embeeding vector. \n",
    "     # The vocab_size is the vocabulary size of the training dataset created by tokenizer during training of corpus dataset in step 2.\n",
    "     self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "     \n",
    " def forward(self, input):\n",
    "     # In addition of feeding input sequence to the embedding layer, the extra multiplication by square root of d_model is done to normalize the embedding layer output\n",
    "     embedding_output = self.embedding(input) * math.sqrt(self.d_model)\n",
    "     return embedding_output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    " def __init__(self, max_seq_len: int, d_model: int, dropout_rate: float):\n",
    "     super().__init__()\n",
    "     self.dropout = nn.Dropout(dropout_rate)\n",
    "     \n",
    "     # We're creating a matrix of the same shape as embedding vector.\n",
    "     pe = torch.zeros(max_seq_len, d_model)\n",
    "     \n",
    "     # Calculate the position part of PE functions.\n",
    "     pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "     # Calculate the division part of PE functions. Take note that the div part expression is slightly different that papers expression as this exponential functions seems to works better.\n",
    "     div_term = torch.exp(torch.arange(0, d_model, 2).float()) * (-math.log(10000)/d_model)\n",
    "     \n",
    "     # Fill in the odd and even matrix value with the sin and cosine mathematical function results.\n",
    "     pe[:, 0::2] = torch.sin(pos * div_term)\n",
    "     pe[:, 1::2] = torch.cos(pos * div_term)\n",
    "     \n",
    "     # Since we're expecting the input sequences in batches so the extra batch_size dimension is added in 0 postion.\n",
    "     pe = pe.unsqueeze(0) \n",
    " \n",
    " def forward(self, input_embdding):\n",
    "     # Add positional encoding together with the input embedding vector.\n",
    "     input_embdding = input_embdding + (self.pe[:, :input_embdding.shape[1], :]).requires_grad_(False) \n",
    "     \n",
    "     # Perform dropout to prevent overfitting.\n",
    "     return self.dropout(input_embdding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00d18712-3ebf-4ee2-8331-d28c19d8f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5: Multi-Head Attention Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bedff450-2549-4065-b067-0bb5c9300aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    " def __init__(self, d_model: int, num_heads: int, dropout_rate: float):\n",
    "     super().__init__()\n",
    "     # Define dropout to prevent overfitting.\n",
    "     self.dropout = nn.Dropout(dropout_rate)\n",
    "     \n",
    "     # Weight matrix are introduced and are all learnable parameters.\n",
    "     self.W_q = nn.Linear(d_model, d_model)\n",
    "     self.W_k = nn.Linear(d_model, d_model)\n",
    "     self.W_v = nn.Linear(d_model, d_model)\n",
    "     self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "     self.num_heads = num_heads\n",
    "     assert d_model % num_heads == 0, \"d_model must be divisible by number of heads\"\n",
    "     \n",
    "     # d_k is the new dimension of each splitted self attention heads\n",
    "     self.d_k = d_model // num_heads\n",
    "\n",
    " def forward(self, q, k, v, encoder_mask=None):\n",
    " \n",
    "     # We'll be training our model with multiple batches of sequence at once in parallel, hence we'll need to include batch_size in the shape as well.\n",
    "     # query, key and value are calculated by matrix multiplication of corresponding weights with the input embeddings. \n",
    "     # Change of shape: q(batch_size, seq_len, d_model) @ W_q(d_model, d_model) => query(batch_size, seq_len, d_model) [same goes to key and value]. \n",
    "     query = self.W_q(q) \n",
    "     key = self.W_k(k)\n",
    "     value = self.W_v(v)\n",
    "    \n",
    "     # Splitting query, key and value into number of heads. d_model is splitted in d_k across 8 heads.\n",
    "     # Change of shape: query(batch_size, seq_len, d_model) => query(batch_size, seq_len, num_heads, d_k) -> query(batch_size,num_heads, seq_len,d_k) [same goes to key and value].\n",
    "     query = query.view(query.shape[0], query.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n",
    "     key = key.view(key.shape[0], key.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n",
    "     value = value.view(value.shape[0], value.shape[1], self.num_heads ,self.d_k).transpose(1,2)\n",
    "    \n",
    "     # :: SELF ATTENTION BLOCK STARTS ::\n",
    "    \n",
    "     # Attention score is calculated to find the similarity or relation between query with key of itself and all other embedding in the sequence.\n",
    "     # Change of shape: query(batch_size,num_heads, seq_len,d_k) @ key(batch_size,num_heads, seq_len,d_k) => attention_score(batch_size,num_heads, seq_len,seq_len).\n",
    "     attention_score = (query @ key.transpose(-2,-1))/math.sqrt(self.d_k)\n",
    "    \n",
    "     # If mask is provided, the attention score needs to modify as per the mask value. Refer to the details in point no 4.\n",
    "     if encoder_mask is not None:\n",
    "         attention_score = attention_score.masked_fill(encoder_mask==0, -1e9)\n",
    "         \n",
    "         # Softmax function calculates the probability distribution among all the attention scores. It assign higher probabiliy value to higher attention score. Meaning more similar tokens get higher probability value.\n",
    "         # Change of shape: same as attention_score\n",
    "         attention_weight = torch.softmax(attention_score, dim=-1)\n",
    "    \n",
    "     if self.dropout is not None:\n",
    "         attention_weight = self.dropout(attention_weight)\n",
    "        \n",
    "         # Final step in Self attention block is, matrix multiplication of attention_weight with Value embedding vector.\n",
    "         # Change of shape: attention_score(batch_size,num_heads, seq_len,seq_len) @ value(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,num_heads, seq_len,d_k)\n",
    "         attention_output = attention_score @ value\n",
    "         \n",
    "         # :: SELF ATTENTION BLOCK ENDS ::\n",
    "        \n",
    "         # Now, all the heads will be combined back to a single head\n",
    "         # Change of shape:attention_output(batch_size,num_heads, seq_len,d_k) => attention_output(batch_size,seq_len,num_heads,d_k) => attention_output(batch_size,seq_len,d_model) \n",
    "         attention_output = attention_output.transpose(1,2).contiguous().view(attention_output.shape[0], -1, self.num_heads * self.d_k)\n",
    "        \n",
    "         # Finally attention_output is matrix multiplied with output weight matrix to give the final Multi-Head attention output. \n",
    "         # The shape of the multihead_output is same as the embedding input\n",
    "         # Change of shape: attention_output(batch_size,seq_len,d_model) @ W_o(d_model, d_model) => multihead_output(batch_size, seq_len, d_model)\n",
    "         multihead_output = self.W_o(attention_output)\n",
    "         \n",
    "         return multihead_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11723c84-bf83-4d51-81fa-cf66846dadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 6: Feedforward Network, Layer Normalization and AddAndNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "334f99d1-5b8c-45e2-b9d2-0297c5d48e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedfoward Network, Layer Normalization and AddAndNorm Block\n",
    "class FeedForward(nn.Module):\n",
    " def __init__(self, d_model: int, d_ff: int, dropout_rate: float):\n",
    "     super().__init__()\n",
    "    \n",
    "     self.layer_1 = nn.Linear(d_model, d_ff)\n",
    "     self.activation_1 = nn.ReLU()\n",
    "     self.dropout = nn.Dropout(dropout_rate)\n",
    "     self.layer_2 = nn.Linear(d_ff, d_model)\n",
    "     \n",
    " def forward(self, input):\n",
    "     return self.layer_2(self.dropout(self.activation_1(self.layer_1(input))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "     def __init__(self, eps: float = 1e-5):\n",
    "         super().__init__()\n",
    "         #Epsilon is a very small value and it plays an important role to prevent potentially division by zero problem.\n",
    "         self.eps = eps\n",
    "        \n",
    "         #Extra learning parameters gamma and beta are introduced to scale and shift the embedding value as the network needed.\n",
    "         self.gamma = nn.Parameter(torch.ones(1))\n",
    "         self.beta = nn.Parameter(torch.zeros(1))\n",
    "         \n",
    "     def forward(self, input):\n",
    "         mean = input.mean(dim=-1, keepdim=True) \n",
    "         std = input.std(dim=-1, keepdim=True) \n",
    "        \n",
    "         return self.gamma * ((input - mean)/(std + self.eps)) + self.beta\n",
    "         \n",
    " \n",
    "class AddAndNorm(nn.Module):\n",
    "     def __init__(self, dropout_rate: float):\n",
    "         super().__init__()\n",
    "         self.dropout = nn.Dropout(dropout_rate)\n",
    "         self.layer_norm = LayerNorm()\n",
    "    \n",
    "     def forward(self, input, sub_layer):\n",
    "         return input + self.dropout(sub_layer(self.layer_norm(input)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f31f6106-65b3-4265-847f-60d069086832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 7: Encoder block and Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc3efc17-d14e-46ea-8e68-fa8f937e549d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    " def __init__(self, multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float):\n",
    "     super().__init__()\n",
    "     self.multihead_attention = multihead_attention\n",
    "     self.feed_forward = feed_forward\n",
    "     self.add_and_norm_list = nn.ModuleList([AddAndNorm(dropout_rate) for _ in range(2)])\n",
    "\n",
    " def forward(self, encoder_input, encoder_mask):\n",
    "     # First AddAndNorm unit taking encoder input from skip connection and adding it with the output of MultiHead attention block.\n",
    "     encoder_input = self.add_and_norm_list[0](encoder_input, lambda encoder_input: self.multihead_attention(encoder_input, encoder_input, encoder_input, encoder_mask))\n",
    "     \n",
    "     # Second AddAndNorm unit taking output of MultiHead attention block from skip connection and adding it with the output of Feedforward layer.\n",
    "     encoder_input = self.add_and_norm_list[1](encoder_input, self.feed_forward)\n",
    "    \n",
    "     return encoder_input\n",
    "\n",
    "class Encoder(nn.Module):\n",
    " def __init__(self, encoderblocklist: nn.ModuleList):\n",
    "     super().__init__()\n",
    "    \n",
    "     # Encoder class is initialized by taking encoderblock list.\n",
    "     self.encoderblocklist = encoderblocklist\n",
    "     self.layer_norm = LayerNorm()\n",
    "\n",
    " def forward(self, encoder_input, encoder_mask):\n",
    "     # Looping through all the encoder block - 6 times.\n",
    "     for encoderblock in self.encoderblocklist:\n",
    "         encoder_input = encoderblock(encoder_input, encoder_mask)\n",
    "        \n",
    "         # Normalize the final encoder block output and return. This encoder output will be used later on as key and value for the cross attention in decoder block.\n",
    "         encoder_output = self.layer_norm(encoder_input)\n",
    "         return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "030fd345-0c2b-43cf-829e-ec4e6de29787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 8: Decoder block, Decoder and Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6b4f133-bcb4-4843-9653-2246ac7b6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    " def __init__(self, masked_multihead_attention: MultiHeadAttention,multihead_attention: MultiHeadAttention, feed_forward: FeedForward, dropout_rate: float):\n",
    "     super().__init__()\n",
    "     self.masked_multihead_attention = masked_multihead_attention\n",
    "     self.multihead_attention = multihead_attention\n",
    "     self.feed_forward = feed_forward\n",
    "     self.add_and_norm_list = nn.ModuleList([AddAndNorm(dropout_rate) for _ in range(3)])\n",
    "\n",
    " def forward(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n",
    "     # First AddAndNorm unit taking decoder input from skip connection and adding it with the output of Masked Multi-Head attention block.\n",
    "     decoder_input = self.add_and_norm_list[0](decoder_input, lambda decoder_input: self.masked_multihead_attention(decoder_input,decoder_input, decoder_input, decoder_mask))\n",
    "     # Second AddAndNorm unit taking output of Masked Multi-Head attention block from skip connection and adding it with the output of MultiHead attention block.\n",
    "     decoder_input = self.add_and_norm_list[1](decoder_input, lambda decoder_input: self.multihead_attention(decoder_input,encoder_output, encoder_output, encoder_mask)) # cross attention\n",
    "     # Third AddAndNorm unit taking output of MultiHead attention block from skip connection and adding it with the output of Feedforward layer.\n",
    "     decoder_input = self.add_and_norm_list[2](decoder_input, self.feed_forward)\n",
    "     return decoder_input\n",
    "\n",
    "class Decoder(nn.Module):\n",
    " def __init__(self,decoderblocklist: nn.ModuleList):\n",
    "     super().__init__()\n",
    "     self.decoderblocklist = decoderblocklist\n",
    "     self.layer_norm = LayerNorm()\n",
    "\n",
    " def forward(self, decoder_input, decoder_mask, encoder_output, encoder_mask):\n",
    "     for decoderblock in self.decoderblocklist:\n",
    "         decoder_input = decoderblock(decoder_input, decoder_mask, encoder_output, encoder_mask)\n",
    "        \n",
    "         decoder_output = self.layer_norm(decoder_input)\n",
    "         return decoder_output\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    " def __init__(self, vocab_size: int, d_model: int):\n",
    "     super().__init__()\n",
    "     self.projection_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    " def forward(self, decoder_output):\n",
    "     # Projection layer first take in decoder output and passed into the linear layer of shape (d_model, vocab_size) \n",
    "     # Change in shape: decoder_output(batch_size, seq_len, d_model) @ linear_layer(d_model, vocab_size) => output(batch_size, seq_len, vocab_size)\n",
    "     output = self.projection_layer(decoder_output)\n",
    "     \n",
    "     # softmax function to output the probability distribution over the vocabulary\n",
    "     return torch.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "db87e1e7-0ceb-49f6-acb8-46da4ff961bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 9: Create and build a Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d11ae4b-bd07-461e-8bec-01e6003ab675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, source_embed: EmbeddingLayer, target_embed: EmbeddingLayer, source_pos: PositionalEncoding, target_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.source_embed = source_embed\n",
    "        self.source_pos = source_pos\n",
    "        self.encoder = encoder\n",
    "\n",
    "        self.target_embed = target_embed\n",
    "        self.target_pos = target_pos\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    def encode(self, encoder_input, encoder_mask):\n",
    "        encoder_input = self.source_embed(encoder_input)\n",
    "        encoder_input = self.source_pos(encoder_input)\n",
    "        encoder_output = self.encoder(encoder_input, encoder_mask)\n",
    "        return encoder_output\n",
    "\n",
    "    def decode(self, encoder_output, encoder_mask, decoder_input, decoder_mask):\n",
    "        decoder_input = self.target_embed(decoder_input)\n",
    "        decoder_input = self.target_pos(decoder_input)\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, encoder_mask, decoder_mask)\n",
    "        return decoder_output\n",
    "\n",
    "    def project(self, decoder_output):\n",
    "        return self.projection_layer(decoder_output)\n",
    "\n",
    "def build_model(source_vocab_size: int, target_vocab_size: int, source_seq_len: int, target_seq_len: int, d_model: int=512, num_blocks: int=6, num_heads: int=8, dropout_rate: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # Create the embedding layers\n",
    "    source_embed = EmbeddingLayer(d_model, source_vocab_size)\n",
    "    target_embed = EmbeddingLayer(d_model, target_vocab_size)\n",
    "\n",
    "    # Create the positional encoding layers\n",
    "    source_pos = PositionalEncoding(d_model, source_seq_len, dropout_rate)\n",
    "    target_pos = PositionalEncoding(d_model, target_seq_len, dropout_rate)\n",
    "\n",
    "    # Create the encoder-block-list\n",
    "    encoderblocklist = []\n",
    "    for _ in range(num_blocks):\n",
    "        multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n",
    "        encoder_block = EncoderBlock(multihead_attention, feed_forward, dropout_rate)\n",
    "        encoderblocklist.append(encoder_block)\n",
    "    # Create the encoder\n",
    "    encoder = Encoder(nn.ModuleList(encoderblocklist))\n",
    "\n",
    "    # Create the decoder-block-list\n",
    "    decoderblocklist = []\n",
    "    for _ in range(num_blocks):\n",
    "        masked_multihead_attention = MultiHeadAttention(d_model,num_heads, dropout_rate)\n",
    "        cross_multihead_attention = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "        feed_forward = FeedForward(d_model, d_ff, dropout_rate)\n",
    "        decoder_block = DecoderBlock(masked_multihead_attention, cross_multihead_attention, feed_forward, dropout_rate)\n",
    "        decoderblocklist.append(decoder_block)\n",
    "    # Create the decoder\n",
    "    decoder = Decoder(nn.ModuleList(decoderblocklist))\n",
    "\n",
    "    # Create the projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, target_vocab_size)\n",
    "\n",
    "    # Now that we've initialized all the required blocks of transformer, we can now inititiate a model\n",
    "    model = Transformer(encoder, decoder, source_embed, target_embed, source_pos, target_pos, projection_layer)\n",
    "\n",
    "    # For the first time, we'll initialize the model parameters using xavier uniform method. Once training begings the parameters will be updated by the network\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e89e9dc-5825-440e-8fe1-f5175d358f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (source_embed): EmbeddingLayer(\n",
      "    (embedding): Embedding(512, 30000)\n",
      "  )\n",
      "  (source_pos): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (encoderblocklist): ModuleList(\n",
      "      (0-5): 6 x EncoderBlock(\n",
      "        (multihead_attention): MultiHeadAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForward(\n",
      "          (layer_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (activation_1): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (add_and_norm_list): ModuleList(\n",
      "          (0-1): 2 x AddAndNorm(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (target_embed): EmbeddingLayer(\n",
      "    (embedding): Embedding(512, 30000)\n",
      "  )\n",
      "  (target_pos): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoderblocklist): ModuleList(\n",
      "      (0-5): 6 x DecoderBlock(\n",
      "        (masked_multihead_attention): MultiHeadAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attention): MultiHeadAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): FeedForward(\n",
      "          (layer_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (activation_1): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (layer_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (add_and_norm_list): ModuleList(\n",
      "          (0-2): 3 x AddAndNorm(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (projection_layer): ProjectionLayer(\n",
      "    (projection_layer): Linear(in_features=30000, out_features=512, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's build the the final model.\n",
    "model = build_model(tokenizer_en.get_vocab_size(), tokenizer_ta.get_vocab_size(),max_seq_len, max_seq_len, d_model=512).to(device)\n",
    "\n",
    "# Let's look at the architecture that we've just build ourself\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2adeac82-40c2-446e-9dde-f0396f883b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 10: Training and validation of our build LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "65caec15-2b11-4e56-904c-73658968bce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 00:   0%|                                                                                                                                                                                    | 0/22702 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected a 'cpu' device type for generator but found 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 126\u001b[0m\n\u001b[1;32m    115\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m    117\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    118\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_step\u001b[39m\u001b[38;5;124m'\u001b[39m: global_step\n\u001b[1;32m    120\u001b[0m         }, model_filename)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# Train our model\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreload_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[58], line 85\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(preload_epoch)\u001b[0m\n\u001b[1;32m     83\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     84\u001b[0m batch_iterator \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batch_iterator:\n\u001b[1;32m     86\u001b[0m     encoder_input \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoder_input\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (b, seq_len)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_input\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# (B, seq_len)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.9/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:385\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_BaseDataLoaderIter\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 385\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SingleProcessDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:658\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loader):\n\u001b[0;32m--> 658\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/llm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:601\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collate_fn \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mcollate_fn\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_sampler)\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_seed \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mpersistent_workers\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected a 'cpu' device type for generator but found 'cuda'"
     ]
    }
   ],
   "source": [
    "def run_validation(model, validation_ds, tokenizer_en, tokenizer_my, max_seq_len, device, print_msg, global_step):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch[\"encoder_input\"].to(device)\n",
    "            encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "            cls_id = tokenizer_ta.token_to_id('[CLS]')\n",
    "            sep_id = tokenizer_ta.token_to_id('[SEP]')\n",
    "\n",
    "            # Computing the output of the encoder for the source sequence\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            # for prediction task, the first token that goes in decoder input is the [CLS] token\n",
    "            decoder_input = torch.empty(1, 1, device=device).fill_(cls_id).type_as(encoder_input).to(device)\n",
    "            # since we need to keep adding the output back to the input until the [SEP] - end token is received.\n",
    "            while True:\n",
    "                # check if the max length is received\n",
    "                if decoder_input.size(1) == max_seq_len:\n",
    "                    break\n",
    "\n",
    "                # recreate mask each time the new output is added the decoder input for next token prediction\n",
    "                decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)\n",
    "\n",
    "                # apply projection only to the next token\n",
    "                out = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "\n",
    "                # apply projection only to the next token\n",
    "                prob = model.project(out[:, -1])\n",
    "\n",
    "                # select the token with highest probablity which is a greedy search implementation\n",
    "                _, next_word = torch.max(prob, dim=1)\n",
    "                decoder_input = torch.cat(\n",
    "                    [decoder_input, torch.empty(1, 1, device=device).type_as(encoder_input).fill_(next_word.item()).to(device)], dim=1\n",
    "                )\n",
    "                # check if the new token is the end of token\n",
    "                if next_word == sep_id:\n",
    "                    break\n",
    "            # final output is the concatinated decoder input till the end token is reached\n",
    "            model_out = decoder_input.squeeze(0)\n",
    "\n",
    "            source_text = batch[\"source_text\"][0]\n",
    "            target_text = batch[\"target_text\"][0]\n",
    "            model_out_text = tokenizer_ta.decode(model_out.detach().cpu().numpy())\n",
    "\n",
    "            # Print the source, target and model output\n",
    "            print_msg('-'*55)\n",
    "            # print_msg(f\"{f'SOURCE: ':>12}{source_text}\")\n",
    "            # print_msg(f\"{f'TARGET: ':>12}{target_text}\")\n",
    "            # print_msg(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n",
    "            print_msg(f'Source Text: {source_text}')\n",
    "            print_msg(f'Target Text: {target_text}')\n",
    "            print_msg(f'Predicted by TamilGPT: {model_out_text}')\n",
    "\n",
    "            if count == 2:\n",
    "                break\n",
    "\n",
    "def train_model(preload_epoch=None):\n",
    "    # The entire training, validation cycle will run for 20 cycles or epochs.\n",
    "    EPOCHS = 10\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Adam is one of the most commonly used optimization algorithms that hold the current state and will update the parameters based on the computed gradients.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, eps=1e-9)\n",
    "\n",
    "    # If the preload_epoch is not none, that means the training will start with the weights, optimizer that has been last saved and start with preload epoch + 1\n",
    "    if preload_epoch is not None:\n",
    "      model_filename = f\"./tamilgpt/model_{preload_epoch}.pt\"\n",
    "      state = torch.load(model_filename)\n",
    "      model.load_state_dict(state['model_state_dict'])\n",
    "      initial_epoch = state['epoch'] + 1\n",
    "      optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "      global_step = state['global_step']\n",
    "\n",
    "    # The CrossEntropyLoss loss function computes the difference between the projection output and target label.\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_en.token_to_id('[PAD]'), label_smoothing=0.1).to(device)\n",
    "\n",
    "    for epoch in range(initial_epoch, EPOCHS):\n",
    "        # torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len)\n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)\n",
    "            target_label = batch['target_label'].to(device) # (B, seq_len)\n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            projection_output = model.project(decoder_output) # (B, seq_len, vocab_size)\n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            loss = loss_fn(projection_output.view(-1, tokenizer_ta.get_vocab_size()), target_label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()\n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # VALIDATION BLOCK STARTS HERE [Runs every epoch after the training block is complete]\n",
    "        run_validation(model, val_dataloader, tokenizer_en, tokenizer_my, max_seq_len, device, lambda msg: batch_iterator.write(msg), global_step)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = f\"./tamilgpt/model_{epoch}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "# Train our model\n",
    "train_model(preload_epoch=None)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431933db-e748-4612-92ec-c020c78f608c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-gpt",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
